{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpNekDvSc+WbUCF8GbU5+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyesunyun/huggingface-lab/blob/main/huggingface_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning HuggingFace ðŸ¤— Transformer Models\n",
        "\n",
        "*Adapted from https://huggingface.co/docs/transformers/en/training, https://huggingface.co/docs/transformers/en/tasks/sequence_classification, and https://huggingface.co/docs/transformers/en/tasks/summarization#load-billsum-dataset*\n",
        "\n",
        "In this lab, we will go over how to fine-tune a model on text classification (sentiment analysis) and summarization tasks.\n",
        "\n",
        "Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique.\n",
        "\n",
        "We will also go over how we can use HuggingFace's [Trainer](https://huggingface.co/docs/transformers/en/trainer) to complete training and evaluation loop for PyTorch models. You only need to pass it the necessary pieces for training (model, tokenizer, dataset, evaluation function, training hyperparameters, etc.), and the Trainer class takes care of the rest. This makes it easier to start training faster without manually writing your own training loop. But at the same time, Trainer is very customizable and offers a ton of training options so you can tailor it to your exact training needs."
      ],
      "metadata": {
        "id": "umL6yxTh2Uof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have the runtime to GPU. You can pick T4 GPU.\n",
        "\n",
        "Run the following cell to verify your GPU setup. You should see information about the GPU available for your session."
      ],
      "metadata": {
        "id": "bXHT-ZkB6JYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "mNMZXgs16KQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Packages\n",
        "\n",
        "This is only needed for Google Colab users."
      ],
      "metadata": {
        "id": "NVkDrkhG6RdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers installation\n",
        "! pip install transformers[torch] datasets evaluate accelerate rouge_score\n",
        "# Install dependencies\n",
        "! pip install torch"
      ],
      "metadata": {
        "id": "s2m-pnEg6Flg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to upload your fine-tuned model to the HuggingFace Hub. Then, I encourage you to create an account and login."
      ],
      "metadata": {
        "id": "rsVZz9Rp638u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment and run the code below if you would like to login to HuggingFace\n",
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "## When prompted, enter your authemtication token to login\n",
        "## You can generate a secret auth token on HuggingFace account\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "WTp5mPge7Bza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification (Sentiment Analysis)\n",
        "\n",
        "In this section, we will finetune [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) on the [IMDb dataset](https://huggingface.co/datasets/stanfordnlp/imdb) to determine whether a movie review is positive or negative."
      ],
      "metadata": {
        "id": "uYVFhL9z5zkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load IMDB dataset"
      ],
      "metadata": {
        "id": "tx95cost8L1o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xozxVE0SW0Rj"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb = load_dataset(\"imdb\")\n",
        "\n",
        "print(f\"Splits of IMDB dataset: {imdb.keys()}\")\n",
        "\n",
        "print(f\"Number of training samples: {len(imdb['train'])}\")\n",
        "print(f\"Number of testing samples: {len(imdb['test'])}\")\n",
        "\n",
        "print(f\"Example data instance: {imdb['test'][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two fields in this dataset:\n",
        "\n",
        "*   `text`: the movie review text.\n",
        "*   `label`: a value that is either 0 for a negative review or 1 for a positive review."
      ],
      "metadata": {
        "id": "YHbAmy-xJVk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to loading datasets that are on HuggingFace's Hub, you can also load local/remote custom data files using `load_dataset` from `datasets` library.\n",
        "\n",
        "For example:\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "\n",
        "# for CSV file\n",
        "# pass your CSV files as a list if you are several CSV files (ex: train.csv, val.csv, test.csv)\n",
        "dataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n",
        "\n",
        "# for JSON file\n",
        "dataset = load_dataset(\"json\", data_files=\"my_file.json\")\n",
        "\n",
        "# for remote JSON file via HTTP\n",
        "base_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": base_url + \"train-v1.1.json\", \"validation\": base_url + \"dev-v1.1.json\"}, field=\"data\")\n",
        "```\n",
        "\n",
        "For more details, please refer to this documentation: https://huggingface.co/docs/datasets/v3.2.0/loading#load"
      ],
      "metadata": {
        "id": "zU0Jkeob9i1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will just use a subset of the dataset so we don't have to wait too long for fine-tuning to finish.\n",
        "from datasets import DatasetDict, Dataset\n",
        "import random\n",
        "\n",
        "# get the first 2500 rows for train and 500 for test\n",
        "train_indices = random.sample(range(len(imdb['train'])), 2500)\n",
        "test_indices = random.sample(range(len(imdb['test'])), 500)\n",
        "imdb_train = imdb['train'].select(train_indices)\n",
        "imdb_test = imdb['test'].select(test_indices)\n",
        "\n",
        "# create a new dataset in DatasetDict object\n",
        "imdb = DatasetDict({\n",
        "    'train': Dataset.from_list(imdb_train),\n",
        "    'test':  Dataset.from_list(imdb_test)\n",
        "})\n",
        "\n",
        "print(f\"Number of training samples: {len(imdb['train'])}\")\n",
        "print(f\"Number of testing samples: {len(imdb['test'])}\")"
      ],
      "metadata": {
        "id": "Lnd6Iuk1JWFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess\n",
        "\n",
        "The next step is to load a DistilBERT tokenizer to preprocess the text field:"
      ],
      "metadata": {
        "id": "2RDejuPy9uI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "uoq3nVni-niR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERTâ€™s maximum input length:"
      ],
      "metadata": {
        "id": "Wq4gCAnt-q7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)"
      ],
      "metadata": {
        "id": "XA66PNwq-qa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the preprocessing function over the entire dataset, use HuggingFace's Datasets map function. You can speed up map by setting `batched=True` to process multiple elements of the dataset at once:"
      ],
      "metadata": {
        "id": "lP6ZvuUG-x7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "fJfiENTx-4FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding). It's more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n",
        "\n",
        "**Note**: Data Collator is used to form a batch by using a list of dataset elements as input."
      ],
      "metadata": {
        "id": "yAaEIm3N-8ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "5FqIfCXK_Nan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate\n",
        "\n",
        "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the accuracy metric (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
      ],
      "metadata": {
        "id": "GdLA9BxI_meU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "# you could also load \"precision\" or \"f1\" as well for this task"
      ],
      "metadata": {
        "id": "hiH2sIS1_5is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then create a function that passes your predictions and labels to `compute` to calculate the accuracy:"
      ],
      "metadata": {
        "id": "lwGYBpldAYtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "yiPmJqspAcED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your compute_metrics function is ready to go now, and you'll return to it when you setup your training."
      ],
      "metadata": {
        "id": "UW-9dyeXAgOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n",
        "\n",
        "Before you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:"
      ],
      "metadata": {
        "id": "c6rdyRcUAiAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ],
      "metadata": {
        "id": "_JpFEaahAsJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're ready to start training your model now! Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/v4.47.1/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
      ],
      "metadata": {
        "id": "IAwzGlurA7WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "DN6DG0msBAGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "\n",
        "1.   Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. At the end of each epoch, the Trainer will evaluate the accuracy and save the training checkpoint. **If you want to push this model to the Hub by setting `push_to_hub=True` (you need to be signed in to Hugging Face to upload your model).**\n",
        "2.   Pass the training arguments to Trainer along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call [train()](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
      ],
      "metadata": {
        "id": "eaHxgxIOBFpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./imdb_distilbert_model/\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\", # you can use wandb, tensorboard, etc to log your train process\n",
        "    push_to_hub=False, # set to True if you want to push this model to the Hub\n",
        ")\n",
        "\n",
        "# Trainer applies dynamic padding by default when you pass tokenizer to it.\n",
        "# In this case, you donâ€™t need to specify a data collator explicitly.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "## Once training is completed, upload your model to the Hub with the push_to_hub() method\n",
        "# trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "WDSegcKZDMFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check with the `evaluate` method that our `Trainer` did reload the best model properly (if it was not the last one):"
      ],
      "metadata": {
        "id": "9eCD-bziNvt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "1-qTsEyONoP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_number = 314"
      ],
      "metadata": {
        "id": "3PeCNTbSffgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "Now that you've finetuned a model, you can use it for reference!\n",
        "\n"
      ],
      "metadata": {
        "id": "Xb_pQumBD-z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # checks if gpu is available\n",
        "pipeline_device = 0 if device == \"cuda\" else -1 # for determining if we want to load model in GPU or CPU"
      ],
      "metadata": {
        "id": "pukP9-hLF3at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
        "\n",
        "## Easiest way to do inference is to use pipeline()\n",
        "from transformers import pipeline\n",
        "\n",
        "##### ADD YOUR CODE BELOW #####\n",
        "# load the fine-tuned model from locally saved model weights\n",
        "# instantiate sentiment-analysis pipeline\n",
        "# do inference on the above text\n"
      ],
      "metadata": {
        "id": "ewK87j5pEt59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using AutoTokenizer and AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "checkpoint_dir = f\"/content/imdb_distilbert_model/checkpoint-{checkpoint_number}\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir).to(device)\n",
        "\n",
        "# Tokenize the text and return PyTorch tensors\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "  # Pass your inputs to the model and return the logits\n",
        "  logits = model(**inputs).logits\n",
        "\n",
        "# Get the class with the highest probability, and use the model's id2label mapping to convert it to a text label\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id]"
      ],
      "metadata": {
        "id": "WRETMTANE6u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarization\n",
        "\n",
        "Summarization creates a shorter version of a document or an article that captures all the important information. Summarization can be:\n",
        "\n",
        "*   Extractive: extract the most relevant information from a document.\n",
        "*   Abstractive: generate new text that captures the most relevant information.\n",
        "\n",
        "This section will show you how to:\n",
        "\n",
        "1. Finetune [T5](https://huggingface.co/google-t5/t5-small) on the California state bill subset of the [BillSum](https://huggingface.co/datasets/billsum) dataset for abstractive summarization.\n",
        "2. Use your finetuned model for inference."
      ],
      "metadata": {
        "id": "aKgTYt6gNYF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load BillSum dataset\n",
        "\n",
        "Start by loading the smaller California state bill subset of the BillSum dataset from the ðŸ¤— Datasets library:"
      ],
      "metadata": {
        "id": "2fYivUmOUW5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
        "\n",
        "print(f\"Number of samples: {len(billsum)}\")"
      ],
      "metadata": {
        "id": "v6t3D9Z1Ubvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/v3.2.0/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
      ],
      "metadata": {
        "id": "L_Dz7_72U27P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "billsum = billsum.train_test_split(test_size=0.2)\n",
        "\n",
        "print(f\"Number of train samples: {len(billsum['train'])}\")\n",
        "print(f\"Number of test samples: {len(billsum['test'])}\")"
      ],
      "metadata": {
        "id": "A3ey6qZZU6bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at an example\n",
        "billsum[\"train\"][0]"
      ],
      "metadata": {
        "id": "3ejgEDPAVMgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two fields that you'll want to use:\n",
        "\n",
        "* `text`: the text of the bill which'll be the input to the model.\n",
        "* `summary`: a condensed version of text which'll be the model target."
      ],
      "metadata": {
        "id": "O7A7IoXQVPkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess\n",
        "\n",
        "The next step is to load a T5 tokenizer to process text and summary:"
      ],
      "metadata": {
        "id": "TdUz0WYGVZhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"google-t5/t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "sALobionVmD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocessing function you want to create needs to:\n",
        "\n",
        "1. Prefix the input with a prompt so T5 knows this is a summarization task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
        "2. Use the keyword `text_target` argument when tokenizing labels.\n",
        "3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter."
      ],
      "metadata": {
        "id": "mSgI6nelVoZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Preprocess function for the summarization dataset.\n",
        "\n",
        "    Args:\n",
        "        examples: dataset examples\n",
        "\n",
        "    Returns:\n",
        "        model_inputs: dictionary of model inputs\n",
        "    \"\"\"\n",
        "    ##### ADD YOUR CODE BELOW #####\n",
        "\n",
        "    # prefix the inputs (examples[\"text\"]) with a prompt so T5 knows this is a summarization task.\n",
        "    # tokenize the inputs using max_length=1024 and truncate\n",
        "    # tokenize the target (examples[\"summary\"]) to max_length=128 and truncate\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "T1_7JJitVuu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the preprocessing function over the entire dataset, use Datasets `map` method. You can speed up the map function by setting `batched=True` to process multiple elements of the dataset at once:"
      ],
      "metadata": {
        "id": "genJuvebXEl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### ADD YOUR CODE BELOW #####"
      ],
      "metadata": {
        "id": "BTFg1HdiXJYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ],
      "metadata": {
        "id": "JcZxHUnJXRMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ],
      "metadata": {
        "id": "f1IDanRKXVgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate\n",
        "\n",
        "Including a metric during training is often helpful for evaluating your model's performance. For this task, load the [ROUGE](https://huggingface.co/spaces/evaluate-metric/rouge) metric."
      ],
      "metadata": {
        "id": "9jryuuGIXY3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "SFNZtA6KXimW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then create a function that passes your predictions and labels to compute to calculate the ROUGE metric:"
      ],
      "metadata": {
        "id": "Pq0kmrFyXlS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # During training, some tokens (typically padding tokens) in the labels might be set to -100.\n",
        "    # The purpose of this line is to replace those -100 values with the actual padding token ID used by the tokenizer.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # stem the text before computing ROUGE\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # calculate the length of each generated summary (padding tokens are not counted)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    # calculate the average length of the generated summaries\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "kXXbnGecXkZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n",
        "\n",
        "Load T5 with AutoModelForSeq2SeqLM:"
      ],
      "metadata": {
        "id": "y8O8pGWJXqWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "##### ADD CODE HERE #####"
      ],
      "metadata": {
        "id": "4fomM2bqXsc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1. Define your training hyperparameters in [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. At the end of each epoch, the Trainer will evaluate the ROUGE metric and save the training checkpoint.\n",
        "2. Pass the training arguments to [Seq2SeqTrainer](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#transformers.Seq2SeqTrainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
        "3. Call `train()` to finetune your model."
      ],
      "metadata": {
        "id": "GJ5eKJ7hXt5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./billsum_t5_model/\"\n",
        "\n",
        "##### ADD CODE HERE #####\n",
        "\n",
        "# add the missing arguments for Seq2SeqTrainingArguments\n",
        "# follow the example from sentiment analysis to help you\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    # ADD more arguments here\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True, #change to bf16=True for XPU\n",
        "    report_to=\"none\", # you can use wandb, tensorboard, etc to log your train process\n",
        "    push_to_hub=False, # set to True if you want to push this model to the Hub\n",
        ")\n",
        "\n",
        "# load Seq2SeqTrainer with the correct arguments\n",
        "# follow example from sentiment analysis\n",
        "# then call train()\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_billsum[\"train\"],\n",
        "    eval_dataset=tokenized_billsum[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# OPTIONAL\n",
        "# trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "X2uqo6TCX8C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "-ceGZsRLcrQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model should be saved in `/content/billsum_t5_model/checkpoint-186`"
      ],
      "metadata": {
        "id": "_73dcQRncvl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "Come up with some text youâ€™d like to summarize. For T5, you need to prefix your input depending on the task youâ€™re working on."
      ],
      "metadata": {
        "id": "a26w4rxtYXi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\""
      ],
      "metadata": {
        "id": "Uz-xgsCgYcvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### ADD CODE HERE #####\n",
        "# either use pipeline() or AutoModelForSeq2SeqLM to do inference\n",
        "# make sure you load the correct checkpoint directory"
      ],
      "metadata": {
        "id": "UfDLOvvAYl3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix\n",
        "\n",
        "### Trainer Basic Tutorial\n",
        "\n",
        "https://huggingface.co/docs/transformers/en/training#train-with-pytorch-trainer\n",
        "\n",
        "### Parameter-Efficient Fine-Tuning\n",
        "\n",
        "Even fine-tuning LLMs can become resource intensive. Hence, some researchers have come up with parameter-efficient fine-tuning (PEFT) methods.\n",
        "\n",
        "LoRa (low-rank adaptation) is one of the most well-known and popular PEFT method. It's a popular method for fine-tuning models in a parameter-efficient way, by only training a few adapters, keeping the existing model untouched. LoRa is available in the [PEFT library](https://huggingface.co/docs/peft/index) by Hugging Face, which also supports various other PEFT methods.\n",
        "\n",
        "QLoRa is another PEFT method that is even more efficient. This method applies LoRa to a quantized model (like 8-bit or 4-bit model).\n",
        "\n",
        "If \"full fine-tuning\" exceeds the GPU RAM, then these training methods can help.\n",
        "\n",
        "### More Examples\n",
        "\n",
        "For more notebookes with examples on fine-tuning, please refer to the following resources:\n",
        "\n",
        "*   https://huggingface.co/docs/transformers/notebooks#pytorch-nlp\n",
        "*   https://github.com/NielsRogge/Transformers-Tutorials\n",
        "\n",
        "### Hyperparameter Search\n",
        "\n",
        "`Trainer` supports four hyperparameter search backends currently: optuna, sigopt, raytune and wandb.\n",
        "\n",
        "https://huggingface.co/docs/transformers/en/hpo_train"
      ],
      "metadata": {
        "id": "xm0i7Wok3MdM"
      }
    }
  ]
}