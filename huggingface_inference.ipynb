{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyesunyun/huggingface-lab/blob/main/huggingface_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw_gevoAVF6E"
      },
      "source": [
        "# HuggingFace Lab\n",
        "\n",
        "*Adapted from https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb*\n",
        "\n",
        "## What is HuggingFace?\n",
        "\n",
        "HuggingFace is an open-source platform that provides tools for building, training, and deploying machine learning (ML) and natural language processing (NLP) models. It is similar to GitHub for AI and is a hub for AI developers.\n",
        "\n",
        "HuggingFace has a large model and datasets library. You can browse and create your own models and share their weights (either as public or private). Also, you can find over 30,000 datasets for training or evaluating AI models.\n",
        "\n",
        "In this lab, we will do a quick tour of using HuggingFace's Transformers & Datasets libraries for different common NLP tasks with pretrained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n4l0-GsVF6G"
      },
      "source": [
        "### Install Packages\n",
        "\n",
        "This is only needed for Google Colab users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2kDXvXqVF6H"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers[torch] datasets\n",
        "# Install dependencies\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHt0uP1tVF6I"
      },
      "source": [
        "### Quick Tour\n",
        "\n",
        "We will start using the [`pipeline()`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) for rapid inference, and then quickly load a pretrained model and tokenizer with an [AutoClass](https://huggingface.co/docs/transformers/main/en/model_doc/auto) to solve text tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGxj_RFFVF6I"
      },
      "source": [
        "#### Pipeline\n",
        "\n",
        "`pipeline()` is the easiest way to use a pretrained model for a given task. It supports many common tasks out-of-the-box:\n",
        "\n",
        "- Sentiment analysis: classify the polarity of a given text.\n",
        "- Text generation (in English): generate text from a given input.\n",
        "- Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.).\n",
        "- Question answering: extract the answer from the context, given some context and a question.\n",
        "- Fill-mask: fill in the blank given a text with masked words.\n",
        "- Summarization: generate a summary of a long sequence of text or document.\n",
        "- Translation: translate text into another language.\n",
        "- Feature extraction: create a tensor representation of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNA3zo2GVF6J"
      },
      "source": [
        "In this example, we will use `pipeline()` for sentiment analysis.\n",
        "\n",
        "Import and load the pipeline.\n",
        "The pipeline downloads and caches a default pretrained model and tokenizer for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPE2ICexVF6L"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3JsPEIsVF6L"
      },
      "outputs": [],
      "source": [
        "classifier(\"We are very happy to show you the HuggingFace's Transformers library.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n1uiohpVF6L"
      },
      "source": [
        "You can also use more than one sentence by passing a list of sentences to the `pipeline()` which resturns a list of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRQ3W7ZhVF6L"
      },
      "outputs": [],
      "source": [
        "results = classifier([\"We are very happy to show you the HuggingFace's Transformers library.\", \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRpmGrRqVF6M"
      },
      "source": [
        "The `pipeline()` can accommodate any model from the Model Hub, making it easy to adapt the `pipeline()`.\n",
        "\n",
        "In this example, the task is translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPr8r2J-VF6M"
      },
      "outputs": [],
      "source": [
        "# Change `xx` to the language of the input and `yy` to the language of the desired output.\n",
        "# Examples: \"en\" for English, \"fr\" for French, \"de\" for German, \"es\" for Spanish, \"zh\" for Chinese, etc; translation_en_to_fr translates English to French\n",
        "# You can view all the lists of languages here - https://huggingface.co/languages\n",
        "\n",
        "# Helsinki-NLP/opus-mt-en-es is the model used for translation from English to Spanish\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "translator = pipeline(\"translation_en_to_es\", model=model_name)\n",
        "\n",
        "text = \"Peanut butter is a food paste or spread made from ground, dry-roasted peanuts.\"\n",
        "translator(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to load the pipeline:\n",
        "Use the AutoModelForSequenceClassification and AutoTokenizer to load the pretrained model and it's associated tokenizer (more on an AutoClass below):\n",
        "\n",
        "```python\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"username/model_name\"\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "pipeline = pipeline(\"task name\", model=model, tokenizer=tokenizer)\n",
        "pipeline(\"text\")\n",
        "```"
      ],
      "metadata": {
        "id": "LpGH5ucwXoRr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv2Z9wJNVF6O"
      },
      "source": [
        "We can also iterate over an entire dataset via HuggingFace's [Datasets](https://huggingface.co/docs/datasets/index) library. We will load [opus-100's en-es test split dataset](https://huggingface.co/datasets/Helsinki-NLP/opus-100/viewer/en-es/test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q3bUk6hVF6O"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Helsinki-NLP/opus-100\", name=\"en-es\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIOL2t5bVF6O"
      },
      "outputs": [],
      "source": [
        "# select first 4 samples from the dataset and format\n",
        "inputs = [sample[\"en\"] for sample in dataset[:4][\"translation\"]]\n",
        "result = translator(inputs)\n",
        "\n",
        "for d in result:\n",
        "  print(d[\"translation_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go6uz64sVF6O"
      },
      "source": [
        "For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the pipeline documentation for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gItBDJH7VF6O"
      },
      "source": [
        "Let's practice with a question answering task but with a model that can handle French text. Search for a model in [Model Hub](https://huggingface.co/models) that handle question answering and French. Tip: Use the tags `Question Answering` NLP task and `French` language.\n",
        "Use the appropriate model to load `pipeline()` and use the dataset: [manu/fquad2_test](https://huggingface.co/datasets/manu/fquad2_test)\n",
        "\n",
        "Dataset Details:\n",
        "- split: test\n",
        "- pre-processing: question-answering pipeline takes in question and context. `qa(question=questions, context=contexts)`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Add your code below #####\n",
        "\n",
        "# load pipeline\n",
        "\n",
        "# load dataset\n",
        "\n",
        "# sample first 4 rows\n",
        "\n",
        "# call qa pipeline with questions and contexts\n",
        "\n",
        "for result in results:\n",
        "  print(result[\"score\"])\n",
        "  if result['score'] < 0.01:\n",
        "      print(\"La rÃ©ponse n'est pas dans le contexte fourni.\") # The answer is not in the context provided.\n",
        "  else :\n",
        "      print(result['answer'])\n",
        "  print(\"----------\")"
      ],
      "metadata": {
        "id": "hoPFnIZfar0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyMkkdDNVF6P"
      },
      "source": [
        "#### AutoClass and AutoTokenizer\n",
        "\n",
        "Under the hood, `pipeline()` is powered by AutoModels and AutoTokenizers. An [AutoClass](https://huggingface.co/docs/transformers/main/en/model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from it's name or path. You only need to select the appropriate AutoClass for your task and it's associated tokenizer with [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer).\n",
        "\n",
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called tokens. There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization here). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.\n",
        "\n",
        "Let's return to our translation example and see how you can use the AutoClass to replicate the results of the pipeline()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju69vkvFVF6P"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer with the AutoTokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOBx4qtwVF6P"
      },
      "source": [
        "Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlpDoTZKVF6P"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer(\"Peanuts are a good source of protein.\")\n",
        "print(encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVMJkGfdVF6P"
      },
      "source": [
        "The tokenizer will return a dictionary containing:\n",
        "\n",
        "input_ids: numerical representions of your tokens.\n",
        "atttention_mask: indicates which tokens should be attended to.\n",
        "Just like the pipeline(), the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqAeMYO6VF6Q"
      },
      "outputs": [],
      "source": [
        "batch = tokenizer(\n",
        "    [\"Is a taco a sandwich?\", \"I like cilantro with my tacos.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGTL4Z3pVF6Q"
      },
      "source": [
        "Read the [preprocessing tutorial](https://huggingface.co/docs/transformers/main/en/preprocessing) for more details about tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfsmtNPYVF6Q"
      },
      "source": [
        "Transformers provides a simple and unified way to load pretrained instances. This means you can load an AutoModel like you would load an AutoTokenizer. The only difference is selecting the correct AutoModel for the task. Since you are doing text summarization (sequence to sequence), load [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAIPOo_pVF6Q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxSPfEkCVF6Q"
      },
      "source": [
        "See the [task summary](https://huggingface.co/docs/transformers/main/en/task_summary) for which AutoModel class to use for which task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UMRqbEYVF6Q"
      },
      "source": [
        "Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding **:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc98Th6OVF6Q"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_HUFAY5VF6Q"
      },
      "source": [
        "The model outputs are tokenized. We need to decode the output to be able to view the output in natural language:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6p6TC8hVF6R"
      },
      "outputs": [],
      "source": [
        "# decode the outputs\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "print([d for d in decoded])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCl0MIWSVF6R"
      },
      "source": [
        "Let's practice with open-ended text generation task using AutoModelForCausalLM and AutoTokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp3i9udyVF6R"
      },
      "outputs": [],
      "source": [
        "#### Add your code below ####\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# import the AutoModelForCausalLM and AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "# Define the input text(s)\n",
        "\n",
        "# Encode the input text(s)\n",
        "\n",
        "# Generate the output(s)\n",
        "\n",
        "# Decode the output(s)\n",
        "\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9w9VZJXVF6R"
      },
      "source": [
        "### Appendix\n",
        "\n",
        "If you would like to learn how to improve open-ended language generation with very little effort, learn about better decoding methods.\n",
        "\n",
        "https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "huggingface",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}