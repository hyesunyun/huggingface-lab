{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyesunyun/huggingface-lab/blob/main/huggingface_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw_gevoAVF6E"
      },
      "source": [
        "# HuggingFace Lab\n",
        "\n",
        "*Adapted from https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb and https://huggingface.co/docs/transformers/en/conversations*\n",
        "\n",
        "## What is HuggingFace?\n",
        "\n",
        "HuggingFace is an open-source platform that provides tools for building, training, and deploying machine learning (ML) and natural language processing (NLP) models. It is similar to GitHub for AI and is a hub for AI developers.\n",
        "\n",
        "HuggingFace has a large model and datasets library. You can browse and create your own models and share their weights (either as public or private). Also, you can find over 30,000 datasets for training or evaluating AI models.\n",
        "\n",
        "In this lab, we will do a quick tour of using HuggingFace's Transformers & Datasets libraries for different common NLP tasks with pretrained models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you have the runtime to GPU. You can pick T4 GPU.\n",
        "\n",
        "Run the following cell to verify your GPU setup.\n",
        "You should see information about the GPU available for your session."
      ],
      "metadata": {
        "id": "8rjh7DRsHgFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "3ivPSO3KHt9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n4l0-GsVF6G"
      },
      "source": [
        "### Install Packages\n",
        "\n",
        "This is only needed for Google Colab users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2kDXvXqVF6H"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers[torch] datasets\n",
        "# Install dependencies\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHt0uP1tVF6I"
      },
      "source": [
        "### Quick Tour\n",
        "\n",
        "We will start using the [`pipeline()`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) for rapid inference, and then quickly load a pretrained model and tokenizer with an [AutoClass](https://huggingface.co/docs/transformers/main/en/model_doc/auto) to solve text tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGxj_RFFVF6I"
      },
      "source": [
        "#### Pipeline\n",
        "\n",
        "`pipeline()` is the easiest way to use a pretrained model for a given task. It supports many common tasks out-of-the-box:\n",
        "\n",
        "- Sentiment analysis: classify the polarity of a given text.\n",
        "- Text generation (in English): generate text from a given input.\n",
        "- Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.).\n",
        "- Question answering: extract the answer from the context, given some context and a question.\n",
        "- Fill-mask: fill in the blank given a text with masked words.\n",
        "- Summarization: generate a summary of a long sequence of text or document.\n",
        "- Translation: translate text into another language.\n",
        "- Feature extraction: create a tensor representation of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNA3zo2GVF6J"
      },
      "source": [
        "In this example, we will use `pipeline()` for sentiment analysis.\n",
        "\n",
        "Import and load the pipeline.\n",
        "The pipeline downloads and caches a default pretrained model and tokenizer for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6TWHNVWk4uh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # checks if gpu is available\n",
        "pipeline_device = 0 if device == \"cuda\" else -1 # for determining if we want to load model in GPU or CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Sentiment Analysis"
      ],
      "metadata": {
        "id": "0vfsBHwOLnCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPE2ICexVF6L"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", device=pipeline_device)\n",
        "# for the device argument, you can also just do \"auto\".\n",
        "# This will let Huggingface decide where to load the model weights.\n",
        "# We manually set the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3JsPEIsVF6L"
      },
      "outputs": [],
      "source": [
        "classifier(\"We are very happy to show you the HuggingFace's Transformers library.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n1uiohpVF6L"
      },
      "source": [
        "You can also use more than one sentence by passing a list of sentences to the `pipeline()` which resturns a list of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRQ3W7ZhVF6L"
      },
      "outputs": [],
      "source": [
        "results = classifier([\"We are very happy to show you the HuggingFace's Transformers library.\", \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRpmGrRqVF6M"
      },
      "source": [
        "The `pipeline()` can accommodate any model from the Model Hub, making it easy to adapt the `pipeline()`.\n",
        "\n",
        "In this example, the task is translation.\n",
        "\n",
        "##### Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPr8r2J-VF6M"
      },
      "outputs": [],
      "source": [
        "# Change `xx` to the language of the input and `yy` to the language of the desired output.\n",
        "# Examples: \"en\" for English, \"fr\" for French, \"de\" for German, \"es\" for Spanish, \"zh\" for Chinese, etc; translation_en_to_fr translates English to French\n",
        "# You can view all the lists of languages here - https://huggingface.co/languages\n",
        "\n",
        "# Helsinki-NLP/opus-mt-en-es is the model used for translation from English to Spanish\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "translator = pipeline(\"translation_en_to_es\", model=model_name, device=pipeline_device)\n",
        "\n",
        "text = \"Peanut butter is a food paste or spread made from ground, dry-roasted peanuts.\"\n",
        "translator(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpGH5ucwXoRr"
      },
      "source": [
        "Another way to load the pipeline:\n",
        "Use the AutoModelForSequenceClassification and AutoTokenizer to load the pretrained model and it's associated tokenizer (more on an AutoClass below):\n",
        "\n",
        "```python\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"username/model_name\"\n",
        "model = AutoModel.from_pretrained(model_name).to(model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "pipeline = pipeline(\"task name\", model=model, tokenizer=tokenizer)\n",
        "pipeline(\"text\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv2Z9wJNVF6O"
      },
      "source": [
        "We can also iterate over an entire dataset via HuggingFace's [Datasets](https://huggingface.co/docs/datasets/index) library. We will load [opus-100's en-es test split dataset](https://huggingface.co/datasets/Helsinki-NLP/opus-100/viewer/en-es/test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q3bUk6hVF6O"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Helsinki-NLP/opus-100\", name=\"en-es\", split=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIOL2t5bVF6O"
      },
      "outputs": [],
      "source": [
        "# select first 4 samples from the dataset and format\n",
        "inputs = [sample[\"en\"] for sample in dataset[:4][\"translation\"]]\n",
        "result = translator(inputs)\n",
        "\n",
        "for d in result:\n",
        "  print(d[\"translation_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go6uz64sVF6O"
      },
      "source": [
        "For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the pipeline documentation for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gItBDJH7VF6O"
      },
      "source": [
        "##### Question Analysis\n",
        "\n",
        "Let's practice with a question answering task but with a model that can handle French text. Search for a model in [Model Hub](https://huggingface.co/models) that handle question answering and French. Tip: Use the tags `Question Answering` NLP task and `French` language.\n",
        "Use the appropriate model to load `pipeline()` and use the dataset: [manu/fquad2_test](https://huggingface.co/datasets/manu/fquad2_test)\n",
        "\n",
        "Dataset Details:\n",
        "- split: test\n",
        "- pre-processing: question-answering pipeline takes in question and context. `qa(question=questions, context=contexts)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoPFnIZfar0U"
      },
      "outputs": [],
      "source": [
        "##### ADD YOUR CODE BELOW #####\n",
        "\n",
        "# load pipeline\n",
        "\n",
        "# load dataset\n",
        "\n",
        "# sample first 4 rows\n",
        "\n",
        "# call qa pipeline with questions and contexts\n",
        "\n",
        "for result in results:\n",
        "  print(result[\"score\"])\n",
        "  if result['score'] < 0.01:\n",
        "      print(\"La réponse n'est pas dans le contexte fourni.\") # The answer is not in the context provided.\n",
        "  else :\n",
        "      print(result['answer'])\n",
        "  print(\"----------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyMkkdDNVF6P"
      },
      "source": [
        "#### AutoClass and AutoTokenizer\n",
        "\n",
        "Under the hood, `pipeline()` is powered by AutoModels and AutoTokenizers. An [AutoClass](https://huggingface.co/docs/transformers/main/en/model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from it's name or path. You only need to select the appropriate AutoClass for your task and it's associated tokenizer with [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer).\n",
        "\n",
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called tokens. There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization here). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.\n",
        "\n",
        "##### Translation\n",
        "\n",
        "Let's return to our translation example and see how you can use the AutoClass to replicate the results of the pipeline()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju69vkvFVF6P"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer with the AutoTokenizer\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOBx4qtwVF6P"
      },
      "source": [
        "Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlpDoTZKVF6P"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer(\"Peanuts are a good source of protein.\").to(device)\n",
        "print(encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVMJkGfdVF6P"
      },
      "source": [
        "The tokenizer will return a dictionary containing:\n",
        "\n",
        "*   input_ids: numerical representions of your tokens.\n",
        "*   atttention_mask: indicates which tokens should be attended to.\n",
        "\n",
        "Just like the pipeline(), the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqAeMYO6VF6Q"
      },
      "outputs": [],
      "source": [
        "batch = tokenizer(\n",
        "    [\"Is a taco a sandwich?\", \"I like cilantro with my tacos.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\",\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGTL4Z3pVF6Q"
      },
      "source": [
        "Read the [preprocessing tutorial](https://huggingface.co/docs/transformers/main/en/preprocessing) for more details about tokenization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfsmtNPYVF6Q"
      },
      "source": [
        "Transformers provides a simple and unified way to load pretrained instances. This means you can load an AutoModel like you would load an AutoTokenizer. The only difference is selecting the correct AutoModel for the task. Since you are doing text summarization (sequence to sequence), load [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAIPOo_pVF6Q"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxSPfEkCVF6Q"
      },
      "source": [
        "See the [task summary](https://huggingface.co/docs/transformers/main/en/task_summary) for which AutoModel class to use for which task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UMRqbEYVF6Q"
      },
      "source": [
        "Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding **:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc98Th6OVF6Q"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(**batch).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_HUFAY5VF6Q"
      },
      "source": [
        "The model outputs are tokenized. We need to decode the output to be able to view the output in natural language:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6p6TC8hVF6R"
      },
      "outputs": [],
      "source": [
        "# decode the outputs\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "print([d for d in decoded])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCl0MIWSVF6R"
      },
      "source": [
        "##### Open-ended Text Generation (Causal Language Modeling)\n",
        "\n",
        "Let's practice with open-ended text generation task using AutoModelForCausalLM and AutoTokenizer.\n",
        "\n",
        "We will use gpt 2 causal language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp3i9udyVF6R"
      },
      "outputs": [],
      "source": [
        "#### ADD YOUR CODE BELOW ####\n",
        "\n",
        "# import the AutoModelForCausalLM and AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "# Define the input text(s)\n",
        "\n",
        "# Encode the input text(s)\n",
        "\n",
        "# Generate the output(s)\n",
        "\n",
        "# Decode the output(s)\n",
        "\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chatting\n",
        "\n",
        "You can also chat with Transformers!\n",
        "\n",
        "Chat models continue chats. This means that you pass them a conversation history, which can be as short as a single user message, and the model will continue the conversation by addings its response."
      ],
      "metadata": {
        "id": "e6ehVCtAHPEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n",
        "]"
      ],
      "metadata": {
        "id": "MSPL8VI9FCnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that in addition to the **user's message**, we added a **system message** at the start of the conversation. Not all chat models support system messages, but when they do, they represent high-level directives about how the model should behave in the conversation. You can use this to guide the model - whether you want short or long responses, lighthearted or serious ones, and so on. If you want the model to do useful work instead of practicing its improv routine, you can either omit the system message or try a terse one such as \"You are a helpful and intelligent AI assistant who responds to user queries.\"\n",
        "\n",
        "The quickest way to continue the chat is using [TextGenerationPipeline](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/pipelines#transformers.TextGenerationPipeline).\n",
        "\n",
        "We will use a 1.7 billion parameter chat model named [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct)"
      ],
      "metadata": {
        "id": "8u5a1vv8LNvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\", \"HuggingFaceTB/SmolLM2-1.7B-Instruct\", torch_dtype=torch.bfloat16, device_map=pipeline_device)\n",
        "response = pipe(chat, max_new_tokens=512)\n",
        "print(response[0]['generated_text'][-1]['content'])"
      ],
      "metadata": {
        "id": "zHYzNtNGMUJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can continue the chat by appending your own response to it. The response object returned by the pipeline actually contains the entire chat so far, so we can simply append a message and pass it back:"
      ],
      "metadata": {
        "id": "LEWk20mGMaxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = response[0]['generated_text']\n",
        "chat.append(\n",
        "    {\"role\": \"user\", \"content\": \"Wait, why are crowds and long lines fun?\"}\n",
        ")\n",
        "response = pipe(chat, max_new_tokens=512)\n",
        "print(response[0]['generated_text'][-1]['content'])"
      ],
      "metadata": {
        "id": "Do0t-FsHMdIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are so many different chat models available on [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending). Choosing a model can be very overwhelming.\n",
        "\n",
        "There are two important considerations when choosing a model:\n",
        "\n",
        "\n",
        "1.   The model's size, which will determine if you can fit in memory and how quickly it will run.\n",
        "2.   The quality of the model's chat output.\n",
        "\n",
        "Without quantization, you should expect to need about 2 bytes of memory per parameter. This means that an “8B” model with 8 billion parameters will need about 16GB of memory just to fit the parameters, plus a little extra for other overhead. Note that it is very common to use quantization techniques to reduce the memory usage per parameter to 8 bits, 4 bits, or even less.\n",
        "\n",
        "Using leaderboards can be a good way to consult which chat models perform well.\n",
        "[OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) and the [LMSys Chatbot Arena Leaderboard](https://chat.lmsys.org/?leaderboard) are two popular leaderboards.\n",
        "There are also [domain specific leaderboards](https://huggingface.co/blog/leaderboard-medicalllm)."
      ],
      "metadata": {
        "id": "fmqAUv7eNsr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Chat Model Exercise\n",
        "\n",
        "For this exercise, find a chat model that you would like to use on [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending). T4 GPU has 16GB of memory so in theory you can load an 8B parameter model in \"bfloat16\" (16 bits precision) using `torch_dtype` arugment like the example above.\n",
        "\n",
        "Load the model either with TextGenerationPipeline or AutoModelForCausalLM (and AutoTokenizer) and start playing around with it.\n",
        "\n",
        "Make sure to set up the input in the chat format if you are using AutoModelForCausalLM\n",
        "\n",
        "```python\n",
        "# Prepare the input as before\n",
        "chat = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n",
        "]\n",
        "# Apply the chat template\n",
        "formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "# Tokenize the chat (This can be combined with the previous step using tokenize=True)\n",
        "inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False)\n",
        "# Move the tokenized inputs to the same device the model is on (GPU/CPU)\n",
        "inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
        "# Generate text from the model\n",
        "outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)\n",
        "print(\"Generated tokens:\\n\", outputs)\n",
        "```"
      ],
      "metadata": {
        "id": "wZhBUPR4RABv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### ADD YOUR CODE HERE #####\n"
      ],
      "metadata": {
        "id": "btKVQ9JOQ78d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9w9VZJXVF6R"
      },
      "source": [
        "### Appendix\n",
        "\n",
        "If you would like to learn how to improve open-ended language generation with very little effort, learn about better decoding methods.\n",
        "\n",
        "https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}